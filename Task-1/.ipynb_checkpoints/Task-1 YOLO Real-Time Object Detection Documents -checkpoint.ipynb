{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3550cf3",
   "metadata": {},
   "source": [
    "#               You Only Look Once: Unified, Real-Time Object Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6537e99e",
   "metadata": {},
   "source": [
    "## Overview: <hr>\n",
    "   In this paper, we introduce YOLO, a novel approach to object detection that differs from previous methods by reframing the task as a regression problem for spatially separated bounding boxes and class probabilities. Unlike traditional approaches that repurpose classifiers for detection, YOLO utilizes a single neural network to predict bounding boxes and class probabilities directly from full images in a single evaluation. This unified architecture allows for end-to-end optimization, resulting in highly efficient performance. Our base YOLO model achieves real-time processing of images at 45 frames per second, while a smaller variant, Fast YOLO, achieves an impressive 155 frames per second while maintaining double the mean Average Precision (mAP) of other real-time detectors. Although YOLO may exhibit more localization errors compared to state-of-the-art systems, it demonstrates reduced false positive predictions on background. Furthermore, YOLO exhibits strong generalization capabilities, outperforming alternative detection methods such as DPM and R-CNN when applied to diverse domains, including artwork."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501c92a4",
   "metadata": {},
   "source": [
    "## Content: <hr>\n",
    "1. Introduction <br>\n",
    "2. Unified Detection  <br>\n",
    "3. Comparison to Other Detection Systems <br>\n",
    "4. Experiments <br>\n",
    "5. Real-Time Detection In The Wild <br>\n",
    "6. Conclusion <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da680c26",
   "metadata": {},
   "source": [
    "## End to End Project Step Using Yolo: <hr>\n",
    "* What is Yolo? <br>\n",
    "* Dependencies <br>\n",
    "* Model hyperparameters <br>\n",
    "* Model definiton <br>\n",
    "* Utility functions <br> \n",
    "* Converting weights to Tensorflow format <br>\n",
    "* Running the model <br>\n",
    "* Video processing <br>\n",
    "* To-Do list <br>\n",
    "* Acknowledgements<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff1dec2",
   "metadata": {},
   "source": [
    "## 1. Introduction <hr>\n",
    "Human beings possess an extraordinary capability to swiftly process visual information, recognizing objects, their spatial arrangements, and interactions with remarkable speed and accuracy. This innate proficiency underpins various activities, from navigating complex environments to interpreting dynamic scenes. In the realm of artificial intelligence and computer vision, achieving comparable levels of efficiency and accuracy in object detection has been a longstanding pursuit. Current detection systems often rely on repurposing classifiers, leveraging techniques such as sliding window approaches or region proposal methods to identify objects within images. However, recent developments in object detection algorithms have shown promising advancements, moving closer towards mimicking the rapid and precise capabilities of the human visual system. This paper delves into the evolution of object detection methodologies, examining the transition from conventional approaches to more sophisticated techniques inspired by human perception. <br>\n",
    "* You Only Look Once (YOLO) is a viral and widely used algorithm [1]. YOLO is famous for its object \n",
    "detection characteristic. In 2015, Redmon et al. gave the introduction of the first YOLO version [2]. In the past \n",
    "years, scholars have published several YOLO subsequent versions described as YOLO V2, YOLO V3, YOLO \n",
    "V4, and YOLO V5 [3-10]. There are a few revised-limited versions, such as YOLO-LITE [11-12]. This research \n",
    "paper only focused on the five main YOLO versions. \n",
    "* This paper will compare the main differences among the five YOLO versions from both conceptual designs \n",
    "and implementations. The YOLO versions are improving, and it is essential to understand the main motivations, \n",
    "features development, limitations, and even relationships for the versions. This reviewing paper will be \n",
    "meaningful and insightful for object detection researchers, especially for beginners. \n",
    "*The following first section will give a version comparing from the technique perspective along with the version \n",
    "similarities. The second section describes them through public data. The insightful results are displayed using \n",
    "both figures and tabular. The two primary analyses are focused on the YOLO trends and YOLO-related queries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb640018",
   "metadata": {},
   "source": [
    "## 2. Unified Detection <hr>\n",
    "The YOLO (You Only Look Once) algorithm revolutionizes the field of object detection by consolidating various components into a single neural network. Unlike traditional methods that involve multiple stages of processing, YOLO considers the entire image at once, leveraging global features to predict bounding boxes and class probabilities for all objects simultaneously. This holistic approach enables the model to reason globally about the image, enhancing its accuracy in detecting objects of varying sizes and complexities.\n",
    "One of the key advantages of YOLO is its ability to maintain real-time speeds while ensuring high average precision. By dividing the input image into a grid structure, YOLO assigns responsibility to individual grid cells for detecting objects whose centers lie within them. Each grid cell predicts multiple bounding boxes along with confidence scores, indicating both the presence of an object and the accuracy of the predicted box. This confidence score is determined by the product of the probability of an object being present (Pr(Object)) and the Intersection over Union (IOU) between the predicted box and the ground truth. <br>\n",
    "The predictions made by YOLO comprise five parameters: the coordinates of the bounding box center (x, y), its width (w), height (h), and the confidence score. These predictions are made relative to the grid cell and the entire image, providing flexibility in capturing object positions and sizes accurately. Additionally, YOLO predicts conditional class probabilities for each grid cell, irrespective of the number of bounding boxes predicted. These probabilities represent the likelihood of each class being present in the detected object within the grid cell. <br>\n",
    "<img src=\"Photo/images.png\" alt=\"Italian Trulli\">\n",
    "During inference, YOLO calculates class-specific confidence scores for each predicted box by combining conditional class probabilities and individual box confidence predictions. This calculation yields scores that encode both the probability of a particular class appearing in the box and how well the predicted box aligns with the actual object. Overall, YOLO's unified approach, grid-based methodology, and efficient inference mechanism make it a versatile and powerful solution for various object detection tasks, ranging from autonomous driving to surveillance systems.  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ff711",
   "metadata": {},
   "source": [
    "## 3. Comparison to Other Detection Systems: <hr>\n",
    "**Traditional Detection Pipelines:**<br>\n",
    "Detection pipelines traditionally involve extracting robust features from images using methods like Haar, SIFT, HOG, or convolutional features, followed by classification or localization using classifiers or localizers. <br>\n",
    "**Comparison with YOLO:**<br>\n",
    "The document compares YOLO with various detection frameworks, highlighting its advantages in terms of speed, accuracy, and simplicity compared to methods like Deformable Parts Models (DPM), R-CNN, and others.<br>\n",
    "**Unified Architecture of YOLO**:<br>\n",
    "YOLO employs a unified architecture where a single convolutional neural network handles feature extraction, bounding box prediction, non-maximal suppression, and contextual reasoning concurrently. This unified approach leads to faster and more accurate detection compared to disjoint pipelines.<br>\n",
    "**Comparison with R-CNN:**<br>\n",
    "YOLO shares similarities with R-CNN, such as proposing bounding boxes and scoring them using convolutional features. However, YOLO imposes spatial constraints on grid cell proposals to mitigate duplicate detections and proposes fewer bounding boxes, leading to improved efficiency.<br>\n",
    "**Speed Improvements Over Traditional Methods:**<br>\n",
    "YOLO and other fast detectors improve upon traditional methods by leveraging neural networks for region proposal and feature extraction, resulting in significant speed improvements while maintaining or enhancing accuracy.<br>\n",
    "**General-purpose Detector:**<br>\n",
    "Unlike detectors optimized for single classes like faces or people, YOLO is a general-purpose detector capable of detecting a variety of objects simultaneously, making it versatile for various applications.<br>\n",
    "**Comparison with MultiBox and OverFeat:**<br>\n",
    "YOLO is compared to MultiBox and OverFeat, highlighting its distinction as a complete detection system capable of general object detection, whereas MultiBox and OverFeat are still parts of larger detection pipelines.<br>\n",
    "**Global Context Reasoning:**<br>\n",
    "The document emphasizes YOLO's ability to reason about global context during detection, contrasting with methods like OverFeat, which only consider local information and require significant post-processing for coherent detections.<br>\n",
    "**Versatility and Complexity:**<br>\n",
    "YOLO's versatility lies in its ability to handle complex detection tasks without the need for extensive post-processing or tuning of individual pipeline components.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f520ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
